{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'load_data' from '/home/camelcars/Documents/ucl2/systemsEng/software_FT/projectsite/common/load_data.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import load_data\n",
    "from load_data import ForamDataSet\n",
    "from importlib import reload\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "reload(load_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # val does not use augmentation\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # test does not use augmentation\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../media'\n",
    "image_datasets = {}\n",
    "image_datasets['train'] = ForamDataSet(csv_file='../train.csv',\n",
    "                                       root_dir='../media',\n",
    "                                       transform=data_transforms['train'])\n",
    "image_datasets['val'] = ForamDataSet(csv_file='../val.csv',\n",
    "                                     root_dir='../media',\n",
    "                                     transform=data_transforms['val'])\n",
    "image_datasets['test'] = ForamDataSet(csv_file='../test.csv',\n",
    "                                     root_dir='../media',\n",
    "                                     transform=data_transforms['test'])                                     \n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frequency(path='../all.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.groupby('species').count()\n",
    "    arr = [tuple((x)) for x in df.to_records()]\n",
    "    arr = sorted(arr, key=lambda x:x[1], reverse=True)\n",
    "    species = [str(i[0]) for i in arr]\n",
    "    frequency = [int(i[1]) for i in arr]\n",
    "    \n",
    "    y_pos = np.arange(len(species))\n",
    "    plt.bar(y_pos, frequency, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, species)\n",
    "    plt.xticks(rotation=80)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Species')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "csv_file = pd.read_csv('../all.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = image_datasets['train']\n",
    "val = image_datasets['val']\n",
    "test = image_datasets['test']\n",
    "assert(train.labels == val.labels == test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     n_train  n_val  n_test\n",
      "species                                    \n",
      "G. sacculifer             96     39      38\n",
      "G. ruber                  91     29      35\n",
      "G. tumida                 89     32      35\n",
      "G. truncatulinoides       83     38      43\n",
      "G. scitula                63     44      30\n",
      "G. siphonifera            58     30      35\n",
      "S. dehiscen               58     38      36\n",
      "N. humerosa               55     27      25\n",
      "N. acostaensis            49     28      21\n",
      "P. obliquiloculata        43     29      35\n",
      "G. ungulata               38     15      19\n",
      "G. elongatus              31     17      21\n",
      "G. crassaformis           30     22      26\n",
      "N. dutertrei              24     16      14\n",
      "O. universa               21      8       5\n",
      "G. ruber pink              4      5       3\n"
     ]
    }
   ],
   "source": [
    "def visualize_categories():\n",
    "    train_csv = pd.read_csv('../train.csv').groupby('species').count()\n",
    "    val_csv = pd.read_csv('../val.csv').groupby('species').count()\n",
    "    test_csv = pd.read_csv('../test.csv').groupby('species').count()\n",
    "    merged_csv = pd.merge(pd.merge(train_csv, val_csv, on='species'), test_csv, on='species')\n",
    "    merged_csv.columns = ['n_train', 'n_val', 'n_test']\n",
    "    print(merged_csv.sort_values(by=['n_train'], ascending=False))\n",
    "visualize_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Training the model\n",
    "# ------------------\n",
    "#\n",
    "# Now, let's write a general function to train a model. Here, we will\n",
    "# illustrate:\n",
    "#\n",
    "# -  Scheduling the learning rate\n",
    "# -  Saving the best model\n",
    "#\n",
    "# In the following, parameter ``scheduler`` is an LR scheduler object from\n",
    "# ``torch.optim.lr_scheduler``.\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    # print(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # print('loss:', loss)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), './resnet18-foram.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ftrs: 512\n",
      "parameters <bound method Module.parameters of ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")>\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.5954 Acc: 0.1964\n",
      "val Loss: 2.0000 Acc: 0.3524\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2.2529 Acc: 0.3036\n",
      "val Loss: 1.7226 Acc: 0.4667\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.9913 Acc: 0.3845\n",
      "val Loss: 1.3765 Acc: 0.5905\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.9066 Acc: 0.4393\n",
      "val Loss: 1.5664 Acc: 0.5333\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.7218 Acc: 0.4762\n",
      "val Loss: 1.0216 Acc: 0.6690\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.6727 Acc: 0.4952\n",
      "val Loss: 1.4205 Acc: 0.6143\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.4453 Acc: 0.5536\n",
      "val Loss: 0.9087 Acc: 0.7214\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.2563 Acc: 0.5869\n",
      "val Loss: 0.6963 Acc: 0.7857\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.1415 Acc: 0.6655\n",
      "val Loss: 0.6586 Acc: 0.7810\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.0885 Acc: 0.6714\n",
      "val Loss: 0.6272 Acc: 0.7929\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.0390 Acc: 0.6821\n",
      "val Loss: 0.6241 Acc: 0.8095\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.9950 Acc: 0.6821\n",
      "val Loss: 0.5947 Acc: 0.8119\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.9917 Acc: 0.6905\n",
      "val Loss: 0.5957 Acc: 0.8119\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.9749 Acc: 0.7143\n",
      "val Loss: 0.5669 Acc: 0.8214\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.9267 Acc: 0.7202\n",
      "val Loss: 0.5662 Acc: 0.8214\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.9794 Acc: 0.7024\n",
      "val Loss: 0.5642 Acc: 0.8262\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.9018 Acc: 0.7298\n",
      "val Loss: 0.5646 Acc: 0.8333\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.9427 Acc: 0.6988\n",
      "val Loss: 0.5486 Acc: 0.8214\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.9457 Acc: 0.6881\n",
      "val Loss: 0.5803 Acc: 0.8214\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.6976\n",
      "val Loss: 0.5524 Acc: 0.8333\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.8573 Acc: 0.7548\n",
      "val Loss: 0.5483 Acc: 0.8357\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.9381 Acc: 0.6952\n",
      "val Loss: 0.5699 Acc: 0.8214\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.9334 Acc: 0.7179\n",
      "val Loss: 0.5581 Acc: 0.8310\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.9034 Acc: 0.7310\n",
      "val Loss: 0.5426 Acc: 0.8357\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.8710 Acc: 0.7512\n",
      "val Loss: 0.5422 Acc: 0.8262\n",
      "\n",
      "Training complete in 50m 39s\n",
      "Best val Acc: 0.835714\n"
     ]
    }
   ],
   "source": [
    "# Generic function to display predictions for a few images\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "######################################################################\n",
    "# Finetuning the convnet\n",
    "# ----------------------\n",
    "#\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "#\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "# model_ft = models.vgg16(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print('num_ftrs:', num_ftrs)\n",
    "print('parameters', model_ft.parameters)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 17)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "# minute.\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "# visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "        AvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 17]           8,721\n",
      "================================================================\n",
      "Total params: 11,185,233\n",
      "Trainable params: 11,185,233\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.67\n",
      "Estimated Total Size (MB): 106.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_ft, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.idx_to_class = {num:species for num,species in enumerate(image_datasets['train'].labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path):\n",
    "    \"\"\"Save a PyTorch model checkpoint\n",
    "    This is better than the default save as you save more info\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): model to save\n",
    "        path (str): location to save model. Must start with `model_name-` and end in '.pth'\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        None, save the `model` to `path`\n",
    "\n",
    "    \"\"\"\n",
    "    print(path.split('-'))\n",
    "    model_name = os.path.basename(path).split('-')[0]\n",
    "    print(model_name)\n",
    "    assert (model_name in ['vgg16', 'resnet50', 'resnet18'\n",
    "                           ]), \"Path must have the correct model name\"\n",
    "\n",
    "    # Basic details\n",
    "    checkpoint = {\n",
    "        'class_to_idx': model.class_to_idx,\n",
    "        'idx_to_class': model.idx_to_class,\n",
    "        'epochs': model.epochs,\n",
    "    }\n",
    "\n",
    "    # Extract the final classifier and the state dictionary\n",
    "    if model_name == 'vgg16':\n",
    "        # Check to see if model was parallelized\n",
    "        if multi_gpu:\n",
    "            checkpoint['classifier'] = model.module.classifier\n",
    "            checkpoint['state_dict'] = model.module.state_dict()\n",
    "        else:\n",
    "            checkpoint['classifier'] = model.classifier\n",
    "            checkpoint['state_dict'] = model.state_dict()\n",
    "\n",
    "    elif model_name == 'resnet18':\n",
    "        if multi_gpu:\n",
    "            checkpoint['fc'] = model.module.fc\n",
    "            checkpoint['state_dict'] = model.module.state_dict()\n",
    "        else:\n",
    "            checkpoint['fc'] = model.fc\n",
    "            checkpoint['state_dict'] = model.state_dict()\n",
    "\n",
    "    # Add the optimizer\n",
    "    checkpoint['optimizer'] = model.optimizer\n",
    "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
    "\n",
    "    # Save the data to the path\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    \"\"\"Load a PyTorch model checkpoint\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        path (str): saved model checkpoint. Must start with `model_name-` and end in '.pth'\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        None, save the `model` to `path`\n",
    "\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    # Changing the output of the final layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, 17)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.idx_to_class = {num:species for num,species in enumerate(image_datasets['train'].labels)}    \n",
    "#     for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#     if model_name == 'vgg16':\n",
    "#         model = models.vgg16(pretrained=True)\n",
    "#         # Make sure to set parameters as not trainable\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         model.classifier = checkpoint['classifier']\n",
    "\n",
    "#     elif model_name == 'resnet18':\n",
    "#         model = models.resnet50(pretrained=True)\n",
    "#         # Make sure to set parameters as not trainable\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "# #         model.fc = checkpoint['fc']\n",
    "\n",
    "#     # Load in the state dict\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     print(f'{total_params:,} total parameters.')\n",
    "#     total_trainable_params = sum(\n",
    "#         p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     print(f'{total_trainable_params:,} total gradient parameters.')\n",
    "\n",
    "#     # Move to gpu\n",
    "#     if multi_gpu:\n",
    "#         model = nn.DataParallel(model)\n",
    "\n",
    "#     if train_on_gpu:\n",
    "#         model = model.to('cuda')\n",
    "\n",
    "#     # Model basics\n",
    "#     model.class_to_idx = checkpoint['class_to_idx']\n",
    "#     model.idx_to_class = checkpoint['idx_to_class']\n",
    "#     model.epochs = checkpoint['epochs']\n",
    "\n",
    "#     # Optimizer\n",
    "#     optimizer = checkpoint['optimizer']\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./resnet18', 'foram.pth']\n",
      "resnet18\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6950f41ffe9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./resnet18-foram.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-0885b9571fad>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Basic details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     checkpoint = {\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;34m'class_to_idx'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;34m'idx_to_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ucl2/systemsEng/software_FT/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "save_checkpoint(model_ft, './resnet18-foram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), './resnet18-foram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-34a779b6a8fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./resnet18-foram.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ft' is not defined"
     ]
    }
   ],
   "source": [
    "model_ft.load_state_dict(torch.load('./resnet18-foram.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    # Resize\n",
    "    img = image.resize((256, 256))\n",
    "\n",
    "    # Center crop\n",
    "    width = 256\n",
    "    height = 256\n",
    "    new_width = 224\n",
    "    new_height = 224\n",
    "\n",
    "    left = (width - new_width) / 2\n",
    "    top = (height - new_height) / 2\n",
    "    right = (width + new_width) / 2\n",
    "    bottom = (height + new_height) / 2\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "\n",
    "    # Convert to numpy, transpose color dimension and normalize\n",
    "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
    "\n",
    "    # Standardization\n",
    "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "    img = img - means\n",
    "    img = img / stds\n",
    "\n",
    "    img_tensor = torch.Tensor(img)\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_tensor(image, ax=None, title=None):\n",
    "    \"\"\"\n",
    "    Imshow for Tensor.\n",
    "    Basicaly reverses the image to tensor process\n",
    "    \"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Set the color channel as the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # Reverse the preprocessing steps\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "\n",
    "    # Clip the image pixel values\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    return ax, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = process_image('../media/G. ruber/0c2434995cae47c392a409114d017e7f.tif')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path, topk=5):\n",
    "    \"\"\"Make a prediction for an image using a trained model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        image_path (str): filename of the image\n",
    "        model (PyTorch model): trained model for inference\n",
    "        topk (int): number of top predictions to return\n",
    "\n",
    "    Returns\n",
    "\n",
    "    \"\"\"\n",
    "    real_class = image_path.split('/')[-2]\n",
    "\n",
    "    # Convert to pytorch tensor\n",
    "    img_tensor = process_image(image_path)\n",
    "\n",
    "    # Resize\n",
    "    if train_on_gpu:\n",
    "        img_tensor = img_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        img_tensor = img_tensor.view(1, 3, 224, 224)\n",
    "\n",
    "    # Set to evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(img_tensor)\n",
    "        ps = torch.exp(out)\n",
    "\n",
    "        # Find the topk predictions\n",
    "        topk, topclass = ps.topk(topk, dim=1)\n",
    "\n",
    "        # Extract the actual classes and probabilities\n",
    "        top_classes = [\n",
    "            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n",
    "        ]\n",
    "        top_p = topk.cpu().numpy()[0]\n",
    "\n",
    "        return img_tensor.cpu().squeeze(), top_p, top_classes, real_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint('./resnet18-foram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.6915, -1.6915, -1.6232,  ..., -1.7767, -1.7767, -1.7767],\n",
       "          [-1.6915, -1.6915, -1.6232,  ..., -1.7767, -1.7767, -1.7767],\n",
       "          [-1.6744, -1.6744, -1.6403,  ..., -1.7767, -1.7767, -1.7767],\n",
       "          ...,\n",
       "          [-1.5721, -1.5721, -1.7256,  ..., -1.8109, -1.7256, -1.7256],\n",
       "          [-1.6573, -1.6573, -1.8109,  ..., -1.7597, -1.6915, -1.6915],\n",
       "          [-1.6573, -1.6573, -1.8109,  ..., -1.7597, -1.6915, -1.6915]],\n",
       " \n",
       "         [[-1.6172, -1.6172, -1.5823,  ..., -1.6695, -1.6869, -1.6869],\n",
       "          [-1.6172, -1.6172, -1.5823,  ..., -1.6695, -1.6869, -1.6869],\n",
       "          [-1.5300, -1.5300, -1.5823,  ..., -1.6869, -1.6869, -1.6869],\n",
       "          ...,\n",
       "          [-1.3730, -1.3730, -1.5300,  ..., -1.6346, -1.6346, -1.6346],\n",
       "          [-1.5126, -1.5126, -1.6346,  ..., -1.5823, -1.5997, -1.5997],\n",
       "          [-1.5126, -1.5126, -1.6346,  ..., -1.5823, -1.5997, -1.5997]],\n",
       " \n",
       "         [[-1.2836, -1.2836, -1.2489,  ..., -1.4225, -1.4051, -1.4051],\n",
       "          [-1.2836, -1.2836, -1.2489,  ..., -1.4225, -1.4051, -1.4051],\n",
       "          [-1.2489, -1.2489, -1.3010,  ..., -1.4051, -1.4051, -1.4051],\n",
       "          ...,\n",
       "          [-1.3704, -1.3704, -1.4399,  ..., -1.3704, -1.3704, -1.3704],\n",
       "          [-1.3878, -1.3878, -1.3878,  ..., -1.3357, -1.3531, -1.3531],\n",
       "          [-1.3878, -1.3878, -1.3878,  ..., -1.3357, -1.3531, -1.3531]]]),\n",
       " array([9382.707   ,  194.7901  ,   33.051926,   31.406715,   30.441666],\n",
       "       dtype=float32),\n",
       " ['G. ruber',\n",
       "  'G. elongatus',\n",
       "  'G. siphonifera',\n",
       "  'G. ungulata',\n",
       "  'P. obliquiloculata'],\n",
       " 'G. ruber')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, '../media/G. ruber/0c2434995cae47c392a409114d017e7f.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"\n",
    "    Compute the topk accuracy(s)\n",
    "    target: the correct answer\n",
    "    \"\"\"\n",
    "    if train_on_gpu:\n",
    "        output = output.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        # Find the predicted classes and transpose\n",
    "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "#         print(pred)\n",
    "        # Determine predictions equal to the targets\n",
    "#         print(target.view(1,-1).expand_as(pred))\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "#         print(correct)\n",
    "        res = []\n",
    "\n",
    "        # For each k, find the percentage of correct\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "#             print(correct_k)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "testiter = iter(dataloaders['test'])\n",
    "# Get a batch of testing images and labels\n",
    "features, targets = next(testiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  8, 10,  5],\n",
      "        [ 5, 13,  0,  8],\n",
      "        [ 3,  7,  6,  7],\n",
      "        [10,  3,  9,  1],\n",
      "        [15,  5,  7, 13]])\n",
      "tensor([[ 1,  8, 10,  7],\n",
      "        [ 1,  8, 10,  7],\n",
      "        [ 1,  8, 10,  7],\n",
      "        [ 1,  8, 10,  7],\n",
      "        [ 1,  8, 10,  7]])\n",
      "tensor([[1, 1, 1, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8)\n",
      "[75.0, 100.0]\n",
      "tensor([ 1,  8, 10,  7])\n"
     ]
    }
   ],
   "source": [
    "if train_on_gpu:\n",
    "    accuracy(model(features.to('cuda')), targets, topk=(1, 5))\n",
    "else:\n",
    "    print(accuracy(model(features), targets, topk=(1, 5)))\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75.0]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(model(features), targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_categories(model, test_loader, criterion, topk=(1, 5)):\n",
    "    \"\"\"Measure the performance of a trained PyTorch model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn for inference\n",
    "        test_loader (PyTorch DataLoader): test dataloader\n",
    "        topk (tuple of ints): accuracy to measure\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        results (DataFrame): results for each category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    classes = []\n",
    "    losses = []\n",
    "    # Hold accuracy results\n",
    "    acc_results = np.zeros((len(test_loader.dataset), len(topk)))\n",
    "    i = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Testing loop\n",
    "        for data, targets in test_loader:\n",
    "\n",
    "            # Tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data, targets = data.to('cuda'), targets.to('cuda')\n",
    "\n",
    "            # Raw model output\n",
    "            out = model(data)\n",
    "            # Iterate through each example\n",
    "            for pred, true in zip(out, targets):\n",
    "                # Find topk accuracy\n",
    "                acc_results[i, :] = accuracy(\n",
    "                    pred.unsqueeze(0), true.unsqueeze(0), topk)\n",
    "                classes.append(model.idx_to_class[true.item()])\n",
    "                # Calculate the loss\n",
    "                loss = criterion(pred.view(1, 17), true.view(1))\n",
    "                losses.append(loss.item())\n",
    "                i += 1\n",
    "\n",
    "    # Send results to a dataframe and calculate average across classes\n",
    "    for i in topk:\n",
    "        print(i)\n",
    "    results = pd.DataFrame(acc_results, columns=['top{i}'.format(i=i) for i in topk])\n",
    "    results['class'] = classes\n",
    "    results['loss'] = losses\n",
    "    results = results.groupby(classes).mean()\n",
    "\n",
    "    return results.reset_index().rename(columns={'index': 'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Evaluate the model on all the training data\n",
    "results = evaluate_for_categories(model, dataloaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  class        top1        top5      loss\n",
      "0       G. crassaformis   34.615385   92.307692  2.093007\n",
      "1          G. elongatus   85.714286  100.000000  0.384988\n",
      "2              G. ruber   85.714286   97.142857  0.627504\n",
      "3         G. ruber pink   66.666667  100.000000  1.030158\n",
      "4         G. sacculifer   92.105263  100.000000  0.197761\n",
      "5            G. scitula   73.333333   83.333333  1.901729\n",
      "6        G. siphonifera   82.857143  100.000000  0.571984\n",
      "7   G. truncatulinoides   93.023256   95.348837  0.381241\n",
      "8             G. tumida  100.000000  100.000000  0.042303\n",
      "9           G. ungulata   84.210526  100.000000  0.723908\n",
      "10       N. acostaensis   80.952381  100.000000  0.433141\n",
      "11         N. dutertrei   85.714286   92.857143  0.683870\n",
      "12          N. humerosa   84.000000   96.000000  0.636962\n",
      "13          O. universa   20.000000  100.000000  3.739821\n",
      "14   P. obliquiloculata   77.142857   91.428571  0.724691\n",
      "15          S. dehiscen   88.888889  100.000000  0.300313\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        result = 0\n",
    "        # Testing loop\n",
    "        for data, targets in test_loader:\n",
    "\n",
    "            # Tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data, targets = data.to('cuda'), targets.to('cuda')\n",
    "\n",
    "            # Raw model output\n",
    "            out = model(data)\n",
    "            print(accuracy(model(data), targets)[0])\n",
    "            result += accuracy(model(features), targets)[0]\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    return result/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n",
      "100.0\n",
      "50.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "25.0\n",
      "75.0\n",
      "100.0\n",
      "50.0\n",
      "75.0\n",
      "75.0\n",
      "50.0\n",
      "100.0\n",
      "25.0\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "50.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "50.0\n",
      "75.0\n",
      "50.0\n",
      "100.0\n",
      "75.0\n",
      "75.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "50.0\n",
      "75.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "50.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "50.0\n",
      "100.0\n",
      "100.0\n",
      "75.0\n",
      "50.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "50.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "50.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "100.0\n",
      "75.0\n",
      "50.0\n",
      "25.0\n",
      "75.0\n",
      "100.0\n",
      "100.0\n",
      "50.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on all the training data\n",
    "results = evaluate_all(model, dataloaders['test'], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.783018867924528\n",
      "421\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(len(pd.read_csv('../test.csv')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft_kernel",
   "language": "python",
   "name": "ft_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
